{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c8e23b-9534-42ab-8eca-71cc96622986",
   "metadata": {},
   "source": [
    "Midterm project - Association Rule Mining\n",
    "Step 1 - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30f54c2-cfbf-4aac-89bc-b5b1d5d026ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(dataset_name):\n",
    "    path = f\"data/{dataset_name}.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded {dataset_name}.csv - {len(df)} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e7582-7ea5-4725-b3a0-cdf1465ad106",
   "metadata": {},
   "source": [
    "Step 2 - Brute Force Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf475a4-2dd9-4ba5-ba73-d5e2f149f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2.1 - Count item combinations of size k\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "def count_itemsets(transactions, k:int) -> Counter:\n",
    "    counts = Counter()\n",
    "    for t in transactions:\n",
    "        if len(t) < k:\n",
    "            continue\n",
    "        for comb in combinations(sorted(t),k):\n",
    "            counts[comb] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdf5bbe-fb49-496f-aff4-bd8587a07ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2.2 - Compute support and keep frequent items\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "def get_frequent_itemsets(transactions, min_support=0.2):\n",
    "    n = len(transactions)\n",
    "    supports = {}\n",
    "    frequent = {}\n",
    "    k = 1\n",
    "\n",
    "    while True:\n",
    "        counts = count_itemsets(transactions, k)\n",
    "        freq_k = []\n",
    "        for itemset, cnt in counts.items():\n",
    "            supp = cnt / n\n",
    "            if supp >= min_support:\n",
    "                freq_k.append(itemset)\n",
    "                supports[itemset] = supp\n",
    "        if not freq_k:\n",
    "            break\n",
    "        frequent[k] = sorted(freq_k, key=lambda s: (-supports[s],s))\n",
    "        k += 1\n",
    "    return frequent, supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519ae252-3ef3-4130-a992-71fe86270ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2.3 - Generate rules (A->B) from frequent itemsets\n",
    "from itertools import combinations\n",
    "\n",
    "def support_fraction(itemset, transactions):\n",
    "    n = len(transactions)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    iset = set(itemset)\n",
    "    return sum(1 for t in transactions if iset.issubset(t)) / n\n",
    "\n",
    "def generate_rules(frequent, supports, transactions, min_conf: float=0.6):\n",
    "    rules = []\n",
    "    for k, itemsets in frequent.items():\n",
    "        if k < 2:\n",
    "            continue\n",
    "        for L in itemsets:\n",
    "            L = tuple(sorted(L))\n",
    "            supp_L = supports.get(L, support_fraction(L, transactions))\n",
    "            items = list(L)\n",
    "\n",
    "            for r in range(1, len(items)):\n",
    "                for A in combinations(items, r):\n",
    "                    A = tuple(sorted(A))\n",
    "                    B = tuple(sorted(set(items) - set(A)))\n",
    "\n",
    "                    supp_A = supports.get(A)\n",
    "                    if supp_A is None:\n",
    "                        supp_A = support_fraction(A, transactions)\n",
    "                    if supp_A == 0:\n",
    "                        continue  \n",
    "\n",
    "                    conf = supp_L / supp_A\n",
    "                    if conf >= min_conf:\n",
    "                        supp_B = supports.get(B)\n",
    "                        if supp_B is None:\n",
    "                            supp_B = support_fraction(B, transactions)\n",
    "                        lift = (conf / supp_B) if supp_B > 0 else None\n",
    "                        rules.append((A, B, supp_L, conf, lift))\n",
    "                        \n",
    "    rules.sort(key=lambda x: (-x[3], -x[2], x[0], x[1]))\n",
    "    return rules\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d139f6aa-a40c-4648-bf6d-6b84f253f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.4 — Brute Force helpers (definitions only; called by the menu)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def run_bruteforce_for_dataset(dataset_name: str, min_support: float, min_conf: float, save: bool = True):\n",
    "    tx = load_transactions_csv(dataset_name)\n",
    "\n",
    "    frequent, supports = get_frequent_itemsets(tx, min_support=min_support)\n",
    "    rules = generate_rules(frequent, supports, tx, min_conf=min_conf)\n",
    "\n",
    "    if save:\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        # frequent itemsets file\n",
    "        freq_rows = []\n",
    "        for k, itemsets in frequent.items():\n",
    "            for it in itemsets:\n",
    "                freq_rows.append([k, \",\".join(it), supports[it]])\n",
    "        pd.DataFrame(freq_rows, columns=[\"k\", \"itemset\", \"support\"]).to_csv(\n",
    "            f\"output/{dataset_name}_frequent_itemsets.csv\", index=False\n",
    "        )\n",
    "        # rules file\n",
    "        rule_rows = [\n",
    "            [\",\".join(A), \",\".join(B), supp, conf, (None if lift is None else float(f\"{lift:.6f}\"))]\n",
    "            for A, B, supp, conf, lift in rules\n",
    "        ]\n",
    "        pd.DataFrame(rule_rows, columns=[\"antecedent\",\"consequent\",\"support\",\"confidence\",\"lift\"]).to_csv(\n",
    "            f\"output/{dataset_name}_rules.csv\", index=False\n",
    "        )\n",
    "\n",
    "    total_itemsets = sum(len(v) for v in frequent.values())\n",
    "    summary_row = {\n",
    "        \"Dataset\": dataset_name.upper(),\n",
    "        \"#Transactions\": len(tx),\n",
    "        \"#Frequent Itemsets\": total_itemsets,\n",
    "        \"#Rules\": len(rules),\n",
    "        \"Time (s)\": None  \n",
    "    }\n",
    "    return summary_row, rules\n",
    "\n",
    "\n",
    "def run_bruteforce_all(datasets, min_support: float, min_conf: float, save: bool = True):\n",
    "    rows = []\n",
    "    all_rules = {} \n",
    "    for ds in datasets:\n",
    "        summary_row, rules = run_bruteforce_for_dataset(ds, min_support=min_support, min_conf=min_conf, save=save)\n",
    "        rows.append(summary_row)\n",
    "        all_rules[ds] = rules\n",
    "    summary_df = pd.DataFrame(rows, columns=[\"Dataset\",\"#Transactions\",\"#Frequent Itemsets\",\"#Rules\",\"Time (s)\"])\n",
    "    return summary_df, all_rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc584632-bc8b-44c9-b7e2-09079a3cd796",
   "metadata": {},
   "source": [
    "Step 3 Apriori & FP-Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f7d278-e8a2-4be1-8c9f-a6f47c4bfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1 – Data preparation helper functions for Apriori & FP-Growth\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "\n",
    "\n",
    "def load_transactions_csv(dataset_name):\n",
    "    path = f\"data/{dataset_name}.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File not found: {path}\")\n",
    "        return []\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    if \"Transaction\" not in df.columns:\n",
    "        print(f\"'Transaction' column missing in {dataset_name}.csv\")\n",
    "        print(\"Columns found:\", df.columns.tolist())\n",
    "        return []\n",
    "\n",
    "    transactions = (\n",
    "        df[\"Transaction\"]\n",
    "        .astype(str)\n",
    "        .str.split(\",\")\n",
    "        .apply(lambda L: [s.strip().lower() for s in L if s.strip()])\n",
    "        .tolist()\n",
    "    )\n",
    "    return transactions\n",
    "\n",
    "\n",
    "def one_hot_encode(transactions):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    return pd.DataFrame(te_ary, columns=te.columns_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d66e2d-6ff5-4caa-bb57-4188173f0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3.2 Apriori definition\n",
    "\n",
    "def run_apriori_for_dataset(dataset_name: str, min_support: float, min_conf: float, save: bool = True):\n",
    "    tx = load_transactions_csv(dataset_name)\n",
    "    df_enc = one_hot_encode(tx)\n",
    "    freq_ap = apriori(df_enc, min_support=min_support, use_colnames=True)\n",
    "    rules_ap = association_rules(freq_ap, metric=\"confidence\", min_threshold=min_conf)\n",
    "    if save:\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        freq_ap.to_csv(f\"output/{dataset_name}_apriori_frequent_itemsets.csv\", index=False)\n",
    "        rules_ap.to_csv(f\"output/{dataset_name}_apriori_rules.csv\", index=False)\n",
    "    return {\n",
    "        \"Dataset\": dataset_name.upper(),\n",
    "        \"#Transactions\": len(tx),\n",
    "        \"#Frequent Itemsets\": len(freq_ap),\n",
    "        \"#Rules\": len(rules_ap)\n",
    "    }, rules_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc2b884-6522-43ee-aa70-b2df99a65c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3.3 Run FP-Growth on one dataset\n",
    "\n",
    "def run_fpgrowth_for_dataset(dataset_name: str, min_support: float, min_conf: float, save: bool = True):\n",
    "    tx = load_transactions_csv(dataset_name)\n",
    "    df_enc = one_hot_encode(tx)\n",
    "    freq_fp = fpgrowth(df_enc, min_support=min_support, use_colnames=True)\n",
    "    rules_fp = association_rules(freq_fp, metric=\"confidence\", min_threshold=min_conf)\n",
    "    if save:\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        freq_fp.to_csv(f\"output/{dataset_name}_fpgrowth_frequent_itemsets.csv\", index=False)\n",
    "        rules_fp.to_csv(f\"output/{dataset_name}_fpgrowth_rules.csv\", index=False)\n",
    "    return {\n",
    "        \"Dataset\": dataset_name.upper(),\n",
    "        \"#Transactions\": len(tx),\n",
    "        \"#Frequent Itemsets\": len(freq_fp),\n",
    "        \"#Rules\": len(rules_fp)\n",
    "    }, rules_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567bad7a-282b-4985-9244-3667db108bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Association Rule Mining Menu ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Dataset (amazon/bestbuy/sephora/target/ikea/all):  ikea\n",
      "Algorithm (brute/apriori/fpgrowth/both/all):  all\n",
      "Minimum support (0–1) [default 0.2]:  0.3\n",
      "Minimum confidence (0–1) [default 0.6]:  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing IKEA (minsup=0.3, minconf=0.5)\n",
      "============================================================\n",
      "Brute Force → itemsets: 11, rules: 5, time=0.0032s\n",
      " output/ikea_frequent_itemsets.csv\n",
      " output/ikea_bruteforce_rules.csv\n",
      "Apriori     → itemsets: 11, rules: 5, time=0.0048s\n",
      " output/ikea_apriori_frequent_itemsets.csv\n",
      " output/ikea_apriori_rules.csv\n",
      "FP-Growth   → itemsets: 11, rules: 5, time=0.0057s\n",
      " output/ikea_fpgrowth_frequent_itemsets.csv\n",
      " output/ikea_fpgrowth_rules.csv\n",
      "\n",
      "=== Summary ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>#Transactions</th>\n",
       "      <th>#Frequent Itemsets</th>\n",
       "      <th>#Rules</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IKEA</td>\n",
       "      <td>Brute Force</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IKEA</td>\n",
       "      <td>Apriori</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IKEA</td>\n",
       "      <td>FP-Growth</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset    Algorithm  #Transactions  #Frequent Itemsets  #Rules  Time (s)\n",
       "0    IKEA  Brute Force             20                  11       5  0.003198\n",
       "1    IKEA      Apriori             20                  11       5  0.004769\n",
       "2    IKEA    FP-Growth             20                  11       5  0.005679"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output/menu_run_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Unified Menu Runner (single interactive cell)\n",
    "\n",
    "import os, time, pandas as pd\n",
    "\n",
    "# ---- Input prompts with validation ----\n",
    "print(\"=== Association Rule Mining Menu ===\")\n",
    "DATASETS = [\"amazon\", \"bestbuy\", \"sephora\", \"target\", \"ikea\"]\n",
    "\n",
    "ds_in = input(\"Dataset (amazon/bestbuy/sephora/target/ikea/all): \").strip().lower()\n",
    "while ds_in not in DATASETS + [\"all\"]:\n",
    "    ds_in = input(\"Please enter one of amazon/bestbuy/sephora/target/ikea/all: \").strip().lower()\n",
    "\n",
    "algo_in = input(\"Algorithm (brute/apriori/fpgrowth/both/all): \").strip().lower()\n",
    "while algo_in not in [\"brute\", \"apriori\", \"fpgrowth\", \"both\", \"all\"]:\n",
    "    algo_in = input(\"Please enter one of brute/apriori/fpgrowth/both/all: \").strip().lower()\n",
    "\n",
    "def ask_float(prompt, lo=0.0, hi=1.0, default=None):\n",
    "    while True:\n",
    "        s = input(prompt).strip()\n",
    "        if s == \"\" and default is not None:\n",
    "            return default\n",
    "        try:\n",
    "            v = float(s)\n",
    "            if lo <= v <= hi:\n",
    "                return v\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"Enter a number in [{lo}, {hi}]\")\n",
    "\n",
    "min_support = ask_float(\"Minimum support (0–1) [default 0.2]: \", 0.0, 1.0, default=0.2)\n",
    "min_conf    = ask_float(\"Minimum confidence (0–1) [default 0.6]: \", 0.0, 1.0, default=0.6)\n",
    "\n",
    "datasets = DATASETS if ds_in == \"all\" else [ds_in]\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "def timed(fn, *args, **kwargs):\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, round(time.perf_counter() - t0, 6)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for ds in datasets:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Processing {ds.upper()} (minsup={min_support}, minconf={min_conf})\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        tx = load_transactions_csv(ds)          \n",
    "        df_enc = one_hot_encode(tx)               \n",
    "\n",
    "        # Decide which to run\n",
    "        run_brute   = algo_in in (\"brute\", \"all\", \"both\")  # 'both' runs Apriori+FP; brute is included if 'all' is selected.\n",
    "        run_ap      = algo_in in (\"apriori\", \"both\", \"all\")\n",
    "        run_fp      = algo_in in (\"fpgrowth\", \"both\", \"all\")\n",
    "\n",
    "        # If 'both', exclude brute (both = Apriori + FP); if 'all', include all 3.\n",
    "        if algo_in == \"both\":\n",
    "            run_brute = False\n",
    "        if algo_in == \"brute\":\n",
    "            run_ap = run_fp = False\n",
    "\n",
    "        # ---- Brute Force (custom) ----\n",
    "        if run_brute:\n",
    "            def _brute():\n",
    "                frequent, supports = get_frequent_itemsets(tx, min_support=min_support)\n",
    "                rules = generate_rules(frequent, supports, tx, min_conf=min_conf)\n",
    "                freq_rows = []\n",
    "                for k, itemsets in frequent.items():\n",
    "                    for it in itemsets:\n",
    "                        freq_rows.append([k, \",\".join(it), supports[it]])\n",
    "                pd.DataFrame(freq_rows, columns=[\"k\",\"itemset\",\"support\"]).to_csv(\n",
    "                    f\"output/{ds}_frequent_itemsets.csv\", index=False\n",
    "                )\n",
    "                rule_rows = [\n",
    "                    [\",\".join(A), \",\".join(B), s, c, (None if L is None else float(f\"{L:.6f}\"))]\n",
    "                    for A,B,s,c,L in rules\n",
    "                ]\n",
    "                pd.DataFrame(rule_rows, columns=[\"antecedent\",\"consequent\",\"support\",\"confidence\",\"lift\"]).to_csv(\n",
    "                    f\"output/{ds}_bruteforce_rules.csv\", index=False\n",
    "                )\n",
    "                return sum(len(v) for v in frequent.values()), len(rules)\n",
    "\n",
    "            (bf_itemsets, bf_rules), t = timed(_brute)\n",
    "            rows.append([ds.upper(), \"Brute Force\", len(tx), bf_itemsets, bf_rules, t])\n",
    "            print(f\"Brute Force → itemsets: {bf_itemsets}, rules: {bf_rules}, time={t:.4f}s\")\n",
    "            print(f\" output/{ds}_frequent_itemsets.csv\")\n",
    "            print(f\" output/{ds}_bruteforce_rules.csv\")\n",
    "\n",
    "        # ---- Apriori (mlxtend) ----\n",
    "        if run_ap:\n",
    "            def _ap():\n",
    "                freq_ap = apriori(df_enc, min_support=min_support, use_colnames=True)\n",
    "                rules_ap = association_rules(freq_ap, metric=\"confidence\", min_threshold=min_conf)\n",
    "                freq_ap.to_csv(f\"output/{ds}_apriori_frequent_itemsets.csv\", index=False)\n",
    "                rules_ap.to_csv(f\"output/{ds}_apriori_rules.csv\", index=False)\n",
    "                return len(freq_ap), len(rules_ap)\n",
    "            (ap_itemsets, ap_rules), t = timed(_ap)\n",
    "            rows.append([ds.upper(), \"Apriori\", len(tx), ap_itemsets, ap_rules, t])\n",
    "            print(f\"Apriori     → itemsets: {ap_itemsets}, rules: {ap_rules}, time={t:.4f}s\")\n",
    "            print(f\" output/{ds}_apriori_frequent_itemsets.csv\")\n",
    "            print(f\" output/{ds}_apriori_rules.csv\")\n",
    "\n",
    "        # ---- FP-Growth (mlxtend) ----\n",
    "        if run_fp:\n",
    "            def _fp():\n",
    "                freq_fp = fpgrowth(df_enc, min_support=min_support, use_colnames=True)\n",
    "                rules_fp = association_rules(freq_fp, metric=\"confidence\", min_threshold=min_conf)\n",
    "                freq_fp.to_csv(f\"output/{ds}_fpgrowth_frequent_itemsets.csv\", index=False)\n",
    "                rules_fp.to_csv(f\"output/{ds}_fpgrowth_rules.csv\", index=False)\n",
    "                return len(freq_fp), len(rules_fp)\n",
    "            (fp_itemsets, fp_rules), t = timed(_fp)\n",
    "            rows.append([ds.upper(), \"FP-Growth\", len(tx), fp_itemsets, fp_rules, t])\n",
    "            print(f\"FP-Growth   → itemsets: {fp_itemsets}, rules: {fp_rules}, time={t:.4f}s\")\n",
    "            print(f\" output/{ds}_fpgrowth_frequent_itemsets.csv\")\n",
    "            print(f\" output/{ds}_fpgrowth_rules.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {ds}: {type(e).__name__}: {e}\")\n",
    "        rows.append([ds.upper(), algo_in.title(), None, None, None, None])\n",
    "        continue\n",
    "\n",
    "# ---- Summary table & save ----\n",
    "summary = pd.DataFrame(rows, columns=[\n",
    "    \"Dataset\",\"Algorithm\",\"#Transactions\",\"#Frequent Itemsets\",\"#Rules\",\"Time (s)\"\n",
    "])\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(summary)\n",
    "summary.to_csv(\"output/menu_run_summary.csv\", index=False)\n",
    "print(\"Saved: output/menu_run_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d5bbb-3d8c-429c-af50-bcad8a957c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
